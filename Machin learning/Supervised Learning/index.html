</html>

<!doctype html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Bootstrap CSS -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
        <!-- main CSS -->
        <link rel="stylesheet" href="./css/supervise.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css">
        <link rel="apple-touch-icon" type="image/png" href="https://cpwebassets.codepen.io/assets/favicon/apple-touch-icon-5ae1a0698dcc2402e9712f7d01ed509a57814f994c660df9f7a952f3060705ee.png" />
        <meta name="apple-mobile-web-app-title" content="CodePen">
        <link rel="shortcut icon" type="image/x-icon" href="https://cpwebassets.codepen.io/assets/favicon/favicon-aec34940fbc1a6e787974dcd360f2c6b63348d4b1f4e06c77743096d55480f33.ico" /><link rel="mask-icon" type="image/x-icon" href="https://cpwebassets.codepen.io/assets/favicon/logo-pin-8f3771b1072e3c38bd662872f6b673a722f4b3ca2421637d5596661b4e2132cc.svg" color="#111" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@800;900&display=swap" rel="stylesheet"> 
        <!--Important link source from https://bootsnipp.com/snippets/ooa9M-->
        <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" id="bootstrap-css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css" />
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
        <link rel="stylesheet" href="./../../css/aos.css" />
        <link rel="stylesheet" href="./../../css/hover.min.css" />

       <title>Ahmad Salehiyan</title>

    </head>

    <body class="dark p-0">






        <section id="first" class="first">
            <div class="container-fluid p-0">
                        <img src="./Blog/Supervise_img.png " class="responsive">
                        <div class="col-12 p-0">
                        <h3 class="page_content">Supervised Learning</h3>
                        <img class="contact_image p-0"  src="./Blog/AI.jpg">
                        

               
            </div>


        </section>

        <section id="second" class="second">
            <div class="container-fluid mt-0">
                <div class="row">
                    <div class="col-sm-10 col-lg-11 ">

                        <h3 class="header text-white">What is supervised learning?</h3>
                            <p class="context text-white">
                                Supervised learning is an approach to creating artificial intelligence (<a href="https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence">AI</a>), where a computer algorithm is trained on input data that has been labeled for a particular output. The model is trained until it can detect the underlying patterns and relationships between the input data and the output labels, enabling it to yield accurate labeling results when presented with never-before-seen data.
                                <br>
                                In contrast to supervised learning is <a href="https://www.techtarget.com/whatis/definition/unsupervised-learning">unsupervised learning</a>. In this approach, the algorithm is presented with unlabeled data and is designed to detect patterns or similarities on its own, a process described in more detail below.
                            </p>                        
                        <h3 class="header text-white">How does supervised learning work?</h3>
                            <p class="context text-white">
                                Like all <a href="https://www.techtarget.com/searchenterpriseai/definition/machine-learning-ML">machine learning</a> algorithms, supervised learning is based on training. During its training phase, the system is fed with labeled data sets, which instruct the system what output is related to each specific input value. The trained model is then presented with test data: This is data that has been labeled, but the labels have not been revealed to the algorithm. The aim of the testing data is to measure how accurately the algorithm will perform on unlabeled data.
                                <br>
                                In <a href="https://www.techtarget.com/searchenterpriseai/definition/neural-network">neural network algorithms</a>, the supervised learning process is improved by constantly measuring the resulting outputs of the model and fine-tuning the system to get closer to its target accuracy. The level of accuracy obtainable depends on two things: the available labeled data and the algorithm that is used. In addition:
                                <br>
                                <ul> 
                                    <li>Training data must be balanced and cleaned. Garbage or duplicate data will skew the AI's understanding -- hence, data scientists must be careful with the data the model is trained on.</li> 
                                    <li>The diversity of the data determines how well the AI will perform when presented with new cases; if there are not enough samples in the training data set, the model will falter and fail to yield reliable answers.</li> 
                                    <li>High accuracy, paradoxically, is not necessarily a good indication; it could also mean the model is suffering from <a href="https://www.techtarget.com/whatis/definition/overfitting">overfitting</a> -- i.e., it is overtuned to its particular training data set. Such a data set might perform well in test scenarios but fail miserably when presented with real-world challenges. To avoid overfitting, it is important that the test data is different from the training data to ensure the model is not drawing answers from its previous experience, but instead that the model's inference is generalized.</li> 
                                    <li>The algorithm, on the other hand, determines how that data can be put in use. For instance, deep learning algorithms can be trained to extract billions of parameters from their data and reach unprecedented levels of accuracy, as demonstrated by <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">OpenAI's GPT-3</a>.</li> 
                                   </ul>
                            </p>    
                        <h3 class="header text-white">Classification algorithms</h3>
                            <p class="context text-white">
                                A classification algorithm aims to sort inputs into a given number of categories or classes, based on the labeled data it was trained on. Classification algorithms can be used for binary classifications such as filtering email into spam or non-spam and categorizing customer feedback as positive or negative. Feature recognition, such as recognizing handwritten letters and numbers or classifying drugs into many different categories, is another classification problem solved by supervised learning.

                            </p>    
                            <h3 class="header text-white">Regression models</h3>
                                <p class="context text-white">
                                    <a href="https://www.datasciencecentral.com/profiles/blogs/comparing-model-evaluation-techniques-part-3-regression-models">Regression tasks</a> are different, as they expect the model to produce a numerical relationship between the input and output data. Examples of regression models include predicting real estate prices based on zip code, or predicting click rates in online ads in relation to time of day, or determining how much customers would be willing to pay for a certain product based on their age.
                                    <br>
                                    Algorithms commonly used in supervised learning programs include the following:
                                    <ul> 
                                        <li><a href="https://en.wikipedia.org/wiki/Linear_regression">Linear Regression</a></li>                                        
                                        <li><a href="https://www.techtarget.com/searchbusinessanalytics/definition/logistic-regression">Logistic Regression</a></li> 
                                        <li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Neural Networks</a></li> 
                                        <li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis</a></li> 
                                        <li><a href="https://www.techtarget.com/whatis/definition/decision-tree">Decision Trees</a></li> 
                                        <li><a href="https://en.wikipedia.org/wiki/Similarity_learning">Similarity Learning</a></li> 
                                        <li><a href="https://www.techtarget.com/whatis/definition/Bayesian-logic">Bayseian Logic</a></li> 
                                        <li><a href="https://www.techtarget.com/whatis/definition/support-vector-machine-SVM">Support Vector Machines (SVMs)</a></li> 
                                        <li><a href="https://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm">Random Forests</a></li> 
                                       </ul>
                                </p>
                                <div class="tab">
                                    <button class="tablinks" onclick="openCity(event, 'Linear Regression')">Linear Regression</button>
                                    <button class="tablinks" onclick="openCity(event, 'Neural Networks')">Neural Networks</button>
                                    <button class="tablinks" onclick="openCity(event, 'Decision Trees')">Decision Trees</button>
                                    <button class="tablinks" onclick="openCity(event, 'Random Forests')">Random Forests</button>

                                </div>
                                  
                                  <div id="Linear Regression" class="tabcontent">

                                    <div class="w3-example badge-light">
                                        <h3>Example</h3>
                                        <p>Import <code class="w3-codespan m-2">scipy</code> and draw the line of Linear Regression:</p>
                                      <div class="w3-code notranslate pythonHigh m-2"><span class="pythoncolor" style="color:black"><span class="pythonnumbercolor" style="color:red">
                                      </span>  <span class="pythonkeywordcolor" style="color:mediumblue">import</span> matplotlib.pyplot <span class="pythonkeywordcolor" style="color:mediumblue">as</span> plt<br><span class="pythonkeywordcolor" style="color:mediumblue">from</span> scipy <span class="pythonkeywordcolor" style="color:mediumblue">import</span> stats<br><span class="pythonnumbercolor" style="color:red">
                                      </span>  <br>x = [<span class="pythonnumbercolor" style="color:red">5</span>,<span class="pythonnumbercolor" style="color:red">7</span>,<span class="pythonnumbercolor" style="color:red">8</span>,<span class="pythonnumbercolor" style="color:red">7</span>,<span class="pythonnumbercolor" style="color:red">2</span>,<span class="pythonnumbercolor" style="color:red">17</span>,<span class="pythonnumbercolor" style="color:red">2</span>,<span class="pythonnumbercolor" style="color:red">9</span>,<span class="pythonnumbercolor" style="color:red">4</span>,<span class="pythonnumbercolor" style="color:red">11</span>,<span class="pythonnumbercolor" style="color:red">12</span>,<span class="pythonnumbercolor" style="color:red">9</span>,<span class="pythonnumbercolor" style="color:red">6</span>]<br>y = <span class="pythonnumbercolor" style="color:red">
                                      </span>  [<span class="pythonnumbercolor" style="color:red">99</span>,<span class="pythonnumbercolor" style="color:red">86</span>,<span class="pythonnumbercolor" style="color:red">87</span>,<span class="pythonnumbercolor" style="color:red">88</span>,<span class="pythonnumbercolor" style="color:red">111</span>,<span class="pythonnumbercolor" style="color:red">86</span>,<span class="pythonnumbercolor" style="color:red">103</span>,<span class="pythonnumbercolor" style="color:red">87</span>,<span class="pythonnumbercolor" style="color:red">94</span>,<span class="pythonnumbercolor" style="color:red">78</span>,<span class="pythonnumbercolor" style="color:red">77</span>,<span class="pythonnumbercolor" style="color:red">85</span>,<span class="pythonnumbercolor" style="color:red">86</span>]<br><br>slope, intercept, r, <span class="pythonnumbercolor" style="color:red">
                                      </span>  p, std_err = stats.linregress(x, y)<br><br><span class="pythonkeywordcolor" style="color:mediumblue">def</span> myfunc(x):<br>&nbsp; <span class="pythonnumbercolor" style="color:red">
                                      </span>  <span class="pythonkeywordcolor" style="color:mediumblue">return</span> slope * x + intercept<br><br>mymodel = <span class="pythonkeywordcolor" style="color:mediumblue">list</span>(<span class="pythonkeywordcolor" style="color:mediumblue">map</span>(myfunc, x))<br><br><span class="pythonnumbercolor" style="color:red">
                                      </span>  plt.scatter(x, y)<br>plt.plot(x, mymodel)<br>plt.show() </span></div>
                                      <h3>Result:</h3>
                                      <p><img src="./Blog/img_linear_regression.png" style="max-width:100%"></p>
                                      

                                      </div>
                                  </div>
                                  
  
                                  
                                  <div id="Neural Networks" class="tabcontent">

                                    <div class="badge-light" style="width: 98%; border: 1px solid #ccc; overflow: auto; padding-bottom: 10px; padding-top: 10px;">
                                        <pre> 
<span style="color: blue;">import</span> numpy <span style="color: blue;">as</span> np
                                        
<span style="color: blue;">class</span> <span style="color: purple;">NeuralNetwork</span>():
                                            
<span style="color: blue;">def</span> __init__(<span style="color: blue;">self</span>):
        <span style="color: red;"># seeding for random number generation</span>
        np.random.seed(<span style="color: blue;">1</span>)                                                
       <span style="color: red;">#converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0</span>
       <span style="color: blue;">self</span>.synaptic_weights = <span style="color: lightblue;">2</span> * np.random.random((<span style="color: lightblue;">3</span>, <span style="color: lightblue;">1</span>)) - 1
                                        
<span style="color: blue;">def</span> sigmoid(<span style="color: blue;">self</span>, x):
        <span style="color: red;">#applying the sigmoid function</span>
        <span style="color: blue;">return</span> <span style="color: lightblue;">1</span> / (<span style="color: lightblue;">1</span> + np.exp(-x))
                                        
<span style="color: blue;">def</span> sigmoid_derivative(<span style="color: blue;">self</span>, x):
        <span style="color: red;">#computing derivative to the Sigmoid function</span>
        <span style="color: blue;">return</span> x * (<span style="color: lightblue;">1</span> - x)
                                        
<span style="color: blue;">def</span> train(<span style="color: blue;">self</span>, training_inputs, training_outputs, training_iterations):
                                                
        <span style="color: red;">#training the model to make accurate predictions while adjusting weights continually</span>
        <span style="color: blue;">for</span> iteration <span style="color: blue;">in</span> range(training_iterations):

        <span style="color: red;">#siphon the training data via  the neuron</span>
        output = <span style="color: blue;">self</span>.think(training_inputs)

        <span style="color: red;">#computing error rate for back-propagation</span>
        error = training_outputs - output

        <span style="color: red;">#performing weight adjustments</span>
        adjustments = np.dot(training_inputs.T, error * <span style="color: blue;">self</span>.sigmoid_derivative(output))
        <span style="color: blue;">self</span>.synaptic_weights += adjustments
        <span style="color: blue;">def</span> think(<span style="color: blue;">self</span>, inputs):
        <span style="color: red;">#passing the inputs via the neuron to get output</span>   
        <span style="color: red;">#converting values to floats</span>
        inputs = inputs.astype(<span style="color: blue;">float</span>)
        output = <span style="color: blue;">self</span>.sigmoid(np.dot(inputs, <span style="color: blue;">self</span>.synaptic_weights))
        <span style="color: blue;">return</span> output
<span style="color: blue;">if</span> __name__ == <span style="color: green;">"__main__"</span>: 

        <span style="color: red;">#initializing the neuron class</span>
        neural_network = <span style="color: purple;">NeuralNetwork</span>()
        <span style="color: blue;">print</span>(<span style="color: green;">"Beginning Randomly Generated Weights: "</span>)
        <span style="color: blue;">print</span>(neural_network.synaptic_weights)

        <span style="color: red;">#training data consisting of 4 examples--3 input values and 1 output</span>
        training_inputs = np.array([[<span style="color: lightblue;">0</span>,<span style="color: lightblue;">0</span>,<span style="color: lightblue;">1</span>],
                                    [<span style="color: lightblue;">1</span>,<span style="color: lightblue;">1</span>,<span style="color: lightblue;">1</span>],
                                    [<span style="color: lightblue;">1</span>,<span style="color: lightblue;">0</span>,<span style="color: lightblue;">1</span>],
                                    [<span style="color: lightblue;">0</span>,<span style="color: lightblue;">1</span>,<span style="color: lightblue;">1</span>]])
                                                
        training_outputs = np.array([[<span style="color: lightblue;">0</span>,<span style="color: lightblue;">1</span>,<span style="color: lightblue;">1</span>,<span style="color: lightblue;">0</span>]]).T
                                                
        <span style="color: red;">#training taking place</span>

        neural_network.train(training_inputs, training_outputs, <span style="color: lightblue;">15000</span>)
                                                
        <span style="color: blue;">print</span>(<span style="color: green;">"Ending Weights After Training: "</span>)
        <span style="color: blue;">print</span>(neural_network.synaptic_weights)
                                                
        user_input_one = str(input(<span style="color: green;">"User Input One: "</span>))
        user_input_two = str(input(<span style="color: green;">"User Input Two: "</span>))
        user_input_three = str(input(<span style="color: green;">"User Input Three: "</span>))
                                                    
        <span style="color: blue;">print</span>(<span style="color: green;">"Considering New Situation: "</span>, user_input_one, user_input_two, user_input_three)
        <span style="color: blue;">print</span>(<span style="color: green;">"New Output data: "</span>)
        <span style="color: blue;">print</span>(neural_network.think(np.array([user_input_one, user_input_two, user_input_three])))
        <span style="color: blue;">print</span>(<span style="color: green;">"Wow, we did it!"</span>)
                                        </pre>
                                        <p class="fs-6  ml-2 text-secondary">*We managed to create a simple neural network. <br>
                                            *The neuron began by allocating itself some random weights. Thereafter, it trained itself using the training examples. <br>
                                            
                                            *Consequently, if it was presented with a new situation [1,0,0], it gave the value of 0.9999584.</p>
                                    </div>
                                    <p class="text-black fs-6 text-secondary">Here is the output for running the code:</p>
                                    <img class="alignnone size-full wp-image-85920" src="https://www.kdnuggets.com/wp-content/uploads/code-output.png" alt="Code Output" width="45%" srcset="https://www.kdnuggets.com/wp-content/uploads/code-output.png 399w, https://www.kdnuggets.com/wp-content/uploads/code-output-300x194.png 300w" sizes="(max-width: 399px) 100vw, 399px">
                                  </div>

                                  <div id="Decision Trees" class="tabcontent text-black">

                                    <h2 id="Motivating-Random-Forests:-Decision-Trees">Motivating Random Forests: Decision Trees<a class="anchor-link" href="#Motivating-Random-Forests:-Decision-Trees">¶</a></h2>

                                    <p>Random forests are an example of an <em>ensemble learner</em> built on decision trees.
                                        For this reason we'll start by discussing decision trees themselves.</p>

                                        <p>Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.
                                            For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:</p>
                                    <img src="./Blog/DS_1.png" alt="">
                                    <p>The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.
                                        The trick, of course, comes in deciding which questions to ask at each step.
                                        In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.
                                        Let's now look at an example of this.</p>
                                        <h3 id="Creating-a-decision-tree">Creating a decision tree<a class="anchor-link" href="#Creating-a-decision-tree">¶</a></h3>

                                        <p>Consider the following two-dimensional data, which has one of four class labels:</p>


                                        <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_blobs</span>
                                            
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'rainbow'</span><span class="p">);</span>
                                            </pre>
                                            </div>
                                            
                                            </div>
                                            <img src="./Blog/DS_2.png">
                                            <p>A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.
                                                This figure presents a visualization of the first four levels of a decision tree classifier for this data:</p>

                                            <img src="./Blog/DS_3.png" style="width: 100%;">
                                            <div class="text_cell_render border-box-sizing rendered_html">
                                                <p>Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.
                                                Except for nodes that contain all of one color, at each level <em>every</em> region is again split along one of the two features.</p>
                                                
                                                </div>

                                                <p>This process of fitting a decision tree to our data can be done in Scikit-Learn with the <code>DecisionTreeClassifier</code> estimator:</p>


                                                <div class="inner_cell">
                                                    <div class="input_area">
                                                <div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
                                                <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                                                </pre></div>
                                                
                                                </div>
                                                </div>

                                                <p>Let's write a quick utility function to help us visualize the output of the classifier:</p>

                                                <div class=" highlight hl-ipython3">
<pre><span></span><span class="k">def</span> <span class="nf">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'rainbow'</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
                                                    
        <span class="c1"># Plot the training points</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
        <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'tight'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
        <span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
        <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
                                                    
        <span class="c1"># fit the estimator</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                                                
        <span class="c1"># Create a color plot with the results</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">contours</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span>
        <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                                
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">)</span>
</pre>
                                                </div>

                                                <div class="text_cell_render border-box-sizing rendered_html">
                                                    <p>Now we can examine what the decision tree classification looks like:</p>
                                                    
                                                    </div>


                                                    <div class="input_area">
                                                        <div class=" highlight hl-ipython3"><pre><span></span><span class="n">visualize_classifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                                                        </pre></div>
                                                        
                                                        </div>
                                                        <img src="./Blog/DS_4.png">
                                                        <div class="text_cell_render border-box-sizing rendered_html">
                                                            <p>If you're running this notebook live, you can use the helpers script included in <a href="06.00-figure-code.html#Helper-Code">The Online Appendix</a> to bring up an interactive visualization of the decision tree building process:</p>
                                                            
                                                            </div>

                                                            <div class="input_area">
                                                                <div class=" highlight hl-ipython3">
<pre><span></span><span class="c1"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">plot_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
                                                                
                                                                </div>

                                                                <img src="./Blog/DS_5.png">

                                                                <p>Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.
                                                                    It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data.
                                                                    That is, this decision tree, even at only five levels deep, is clearly over-fitting our data.</p>

                                                                    <h3 id="Decision-trees-and-over-fitting">Decision trees and over-fitting<a class="anchor-link" href="#Decision-trees-and-over-fitting">¶</a></h3>

                                                                    <p>Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.
                                                                        Another way to see this over-fitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data:</p>
                                                                        <img src="./Blog/DS_6.png" style="width: 100%;">

                                                                        <p>It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).
                                                                            The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from <em>both</em> of these trees, we might come up with a better result!</p>
                                                                            <div class="text_cell_render border-box-sizing rendered_html">
                                                                                <p>If you are running this notebook live, the following function will allow you to interactively display the fits of trees trained on a random subset of the data:</p>
                                                                                
                                                                                </div>
                                                                                <div class="input_area">
                                                                                    <div class=" highlight hl-ipython3">
<pre><span></span><span class="c1"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">randomized_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre>
                                                                                    </div>
                                                                                    
                                                                                    </div>

                                                                                    <img src="./Blog/DS_7.png">

                                                                                    <div class="text_cell_render border-box-sizing rendered_html">
                                                                                        <p>Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further.</p>
                                                                                        
                                                                                        </div>
                                                                        

                                                                        <div class="input_area">
<div class=" highlight hl-ipython3">
    <pre><span></span><span class="c1"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n pb-2">helpers_05_08</span><span class="o">.</span><span class="n">randomized_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre>
</div>
                                                                            
                                                                            </div>



                                  </div>
                                  

                                  
                                  <div id="Random Forests" class="tabcontent">
                                    <h2 class="header text-black">Ensembles of Estimators: Random Forests</h2>
                                    <p class="text-black">This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called <em>bagging</em>.
                                        Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification.
                                        An ensemble of randomized decision trees is known as a <em>random forest</em>.</p>

                                        <p class="text-black">This type of bagging classification can be done manually using Scikit-Learn's <code>BaggingClassifier</code> meta-estimator, as shown here:</p>

                                        <div class="input_area">
                                            <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">BaggingClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">bag</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                                            
<span class="n">bag</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">bag</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
                                            
                                            </div>
                                            <img src="./Blog/RF_1.png" alt="">

                                            <p class="text-black">In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.
                                                In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.
                                                For example, when determining which feature to split on, the randomized tree might select from among the top several features.
                                                You can read more technical details about these randomization strategies in the <a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">Scikit-Learn documentation</a> and references within.</p>
                                                <p class="text-black">In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the <code>RandomForestClassifier</code> estimator, which takes care of all the randomization automatically.
                                                    All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:</p>

                                                    <div class="input_area">
                                                        <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
                                                        
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
                                                        
                                                        </div>
                                                        <img src="./Blog/RF_2.png" alt="">

                                                        <p class="text-black">We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split.</p>

                                                    <p class="text-black">We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split.</p>
                                                    <h2 id="Random-Forest-Regression" class="header text-black">Random Forest Regression<a class="anchor-link" href="#Random-Forest-Regression">¶</a></h2>

                                                    <p class="text-black">In the previous section we considered random forests within the context of classification.
                                                        Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the <code>RandomForestRegressor</code>, and the syntax is very similar to what we saw earlier.</p>

                                                        <p class="text-black">Consider the following data, drawn from the combination of a fast and slow oscillation:</p>
                                                        <div class="input_area">
                                                            <div class=" highlight hl-ipython3">
<pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
                                                            
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
<span class="n">fast_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">slow_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                                                            
<span class="k">return</span> <span class="n">slow_oscillation</span> <span class="o">+</span> <span class="n">fast_oscillation</span> <span class="o">+</span> <span class="n">noise</span>
                                                            
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'o'</span><span class="p">);</span>
</pre></div>
                                                            
                                                            </div>
                                                            <img src="./Blog/RF_3.png" alt="">

                                                            <div class="text_cell_render border-box-sizing rendered_html">
                                                                <p class="text-black">Using the random forest regressor, we can find the best fit curve as follows:</p>
                                                                
                                                                </div>

                                                                <div class="input_area">
                                                                    <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
                                                                    
<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">ytrue</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                                                                    
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">,</span> <span class="s1">'-r'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">ytrue</span><span class="p">,</span> <span class="s1">'-k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
                                                                    
                                                                    </div>
                                                                    <img src="./Blog/RF_4.png" alt="">
                                                                    <p class="text-black">Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.
                                                                        As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model!</p>

                                                                        <h2  class="header text-black" id="Example:-Random-Forest-for-Classifying-Digits">Example: Random Forest for Classifying Digits<a class="anchor-link" href="#Example:-Random-Forest-for-Classifying-Digits">¶</a></h2>

                                                                        <p class="text-black">Earlier we took a quick look at the hand-written digits data (see <a href="05.02-introducing-scikit-learn.html">Introducing Scikit-Learn</a>).
                                                                            Let's use that again here to see how the random forest classifier can be used in this context.</p>

                                                                            <div class="input_area">
                                                                                <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
                                                                                
</div>
<pre>dict_keys(['target', 'data', 'target_names', 'DESCR', 'images'])</pre>

                                                                                <div class="text_cell_render border-box-sizing rendered_html">
                                                                                    <p class="header text-black">To remind us what we're looking at, we'll visualize the first few data points:</p>
                                                                                    
                                                                                    </div>
                                                                                    <div class="input_area">
                                                                                        <div class=" highlight hl-ipython3">
<pre><span></span><span class="c1"># set up the figure</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  <span class="c1"># figure size in inches</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
                                                                                        
<span class="c1"># plot the digits: each image is 8x8 pixels</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
                                                                                            
<span class="c1"># label the image with the target value</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
                                                                                        
                                                                                        </div>
                                                                                        <img src="./Blog/RF_5.png" alt="">
                                                                                        <div class="text_cell_render border-box-sizing rendered_html">
                                                                                            <p class="header text-black">We can quickly classify the digits using a random forest as follows:</p>
                                                                                            
                                                                                            </div>

                                                                                            <div class="input_area">
                                                                                                <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
                                                                                                
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">ypred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</pre></div>
                                                                                                
                                                                                                </div>
                                                                                                <div class="text_cell_render border-box-sizing rendered_html">
                                                                                                    <p class="text-black">We can take a look at the classification report for this classifier:</p>
                                                                                                    
                                                                                                    </div>
                                                                                                    <div class="input_area">
                                                                                                        <div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
                                                                                                        <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">))</span>
</pre></div>
                                                                                                        
</div>

<pre>    precision    recall  f1-score   support

 0       1.00      0.97      0.99        38
 1       1.00      0.98      0.99        44
 2       0.95      1.00      0.98        42
 3       0.98      0.96      0.97        46
 4       0.97      1.00      0.99        37
 5       0.98      0.96      0.97        49
 6       1.00      1.00      1.00        52
 7       1.00      0.96      0.98        50
 8       0.94      0.98      0.96        46
 9       0.96      0.98      0.97        46
                                                                                                  
avg / total
         0.98       0.98       0.98      450
                                                                                                  
</pre>

                                                                                                  <div class="text_cell_render border-box-sizing rendered_html">
                                                                                                    <p class="header text-black">And for good measure, plot the confusion matrix:</p>
                                                                                                    
                                                                                                    </div>

                                                                                                    <div class="input_area">
                                                                                                        <div class=" highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'d'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'true label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'predicted label'</span><span class="p">);</span>
</pre></div>
                                                                                                        
                                                                                                        </div>

                                                                                                        <img src="./Blog/RF_6.png" alt="">
                                                                                                        <div class="text_cell_render border-box-sizing rendered_html">
                                                                                                            <p class=" text-black">* We find that a simple, untuned random forest results in a very accurate classification of the digits data.</p>
                                                                                                            
                                                                                                            </div>
                                    

                                  </div>
                                  
                                <p class="context text-white font-italic mt-4">
                                    When choosing a supervised learning algorithm, there are a few things that should be considered. The first is the <a href="https://www.techtarget.com/searchenterpriseai/definition/machine-learning-bias-algorithm-bias-or-AI-bias">bias</a> and variance that exist within the algorithm, as there is a fine line between being flexible enough and too flexible. Another is the complexity of the model or function that the system is trying to learn. As noted, the heterogeneity, accuracy, redundancy and linearity of the data should also be analyzed before choosing an algorithm.
                                </p>
                                                   
                    </div>  





                 
                </div>
               
            </div>


        </section>



 



         
        <footer id="footer" class="footer">
            <div class="row">
                <div class="f_about col-lg-4 col-sm-12">
                    <div class="contact_about_card">
                        <img class="contact_image" src="./images/contact_img.png">
                    </div>                        
                        <p class="f_name">Ahmad Salehiyan</p>
                     
                        <p class="f_job">Industrial Engineer</p>

                    
                </div>
                <div class="col-lg-4 col-sm-12">
                    <h1 class="Contact fw-bold order-sm-3 order-lg-2">Contact With Me:</h1>
                    <div id="social-platforms" class="social-platforms mt-4 ">                          
                        <a class="btn btn-icon btn-telegram hvr-grow" href="https://t.me/AhmadSalehiyan"><i class="fa fa-telegram"></i><span>Telegram</span></a>
                        <a class="btn btn-icon btn-whatsApp  hvr-grow" href="https://wa.me/989398006775"><i class="fa fa-whatsApp"></i><span>WhatsApp</span></a>
                        <a class="btn btn-icon btn-linkedin  hvr-grow" href="http://www.linkedin.com/in/ahmad-salehiyan"><i class="fa fa-linkedin"></i><span>LinkedIn</span></a>
                        </div>
                    <div class="copy_text">
                        <p class="copy  text-white"><span>&#174;</span> Ahmad Salehiyan All Rights Reserved.</p>
                    </div>

                </div>
                <div class="Contact_way col-lg-4 col-sm-12">
                    <h6 class="text_contact fw-bold">
                        Contact
                      </h6>
                      <a href="mailto: Ahamd.Salehiyan@okstate.edu" class="contact_ican_email pb-3"><i class="bi bi-envelope"></i><span>  Ahamd.Salehiyan@okstate.edu</span></i></a>
                      <a href="tel:+405 269 3549" class="contact_ican_call"><i class="bi bi-telephone"></i><span>  +405 269 3549</span></a>
                </div>
          


            </div>






        </footer>
        
        

          

        <script>
            window.console = window.console || function(t) {};
          </script>
          
            
            
            <script>
            if (document.location.search.match(/type=embed/gi)) {
              window.parent.postMessage("resize", "*");
            }
          </script>
        <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>
        <script src="./../../js/aos.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js"></script>
        <script src="./Java.js"></script>
        <script src='https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js'></script>
        <script src='https://maxcdn.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js'></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    </body>

  
</html>


